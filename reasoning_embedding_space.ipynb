{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836916fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1661e",
   "metadata": {},
   "source": [
    "\n",
    "### TENSOR LOGIC Experiment 2 : REASONING WITH DOT PRODUCTS\n",
    "### Aligned with Section 5: 'Reasoning in Embedding Space' \n",
    "### Dataset: mledoze/countries (Public Domain) url: https://raw.githubusercontent.com/mledoze/countries/master/countries.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e37b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0fa7194",
   "metadata": {},
   "source": [
    "#### LOAD & PROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81bd774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data for 250 countries.\n",
      "\n",
      "Extracting Facts...\n",
      "Entities: 489\n",
      "Facts: 490\n"
     ]
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/mledoze/countries/master/countries.json\"\n",
    "data = requests.get(url).json()\n",
    "print(f\"Downloaded data for {len(data)} countries.\")\n",
    "\n",
    "triples = []\n",
    "entities = set()\n",
    "relations = [\"is_capital_of\", \"is_located_in\"]\n",
    "\n",
    "print(\"\\nExtracting Facts...\")\n",
    "for entry in data:\n",
    "    country = entry.get(\"name\", {}).get(\"common\", \"\")\n",
    "    capital_list = entry.get(\"capital\", [])\n",
    "    capital = capital_list[0] if capital_list else None\n",
    "    region = entry.get(\"region\", \"\")\n",
    "\n",
    "    if country and capital and region:\n",
    "        entities.add(country)\n",
    "        entities.add(capital)\n",
    "        entities.add(region)\n",
    "        # Fact 1: Capital -> Country\n",
    "        triples.append((capital, \"is_capital_of\", country))\n",
    "        # Fact 2: Country -> Region\n",
    "        triples.append((country, \"is_located_in\", region))\n",
    "\n",
    "# Index Maps\n",
    "entity_list = sorted(list(entities))\n",
    "ent_to_idx = {e: i for i, e in enumerate(entity_list)}\n",
    "idx_to_ent = {i: e for i, e in enumerate(entity_list)}\n",
    "rel_to_idx = {r: i for i, r in enumerate(relations)}\n",
    "\n",
    "NUM_ENTITIES = len(entities)\n",
    "NUM_RELATIONS = len(relations)\n",
    "\n",
    "print(f\"Entities: {NUM_ENTITIES}\")\n",
    "print(f\"Facts: {len(triples)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae2891",
   "metadata": {},
   "source": [
    "#### We model truth as the dot product between prediction and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddaec8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorLogicDotProduct(nn.Module):\n",
    "    def __init__(self, num_entities, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        # 1. Entity Embeddings\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        \n",
    "        # 2. Relation Matrices (Transformation Tensors)\n",
    "        self.relation_matrices = nn.Parameter(torch.randn(NUM_RELATIONS, embedding_dim, embedding_dim))\n",
    "        \n",
    "        # Initialization\n",
    "        nn.init.xavier_uniform_(self.relation_matrices)\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight)\n",
    "        \n",
    "        # Normalize embeddings to unit vectors (prevents hubness)\n",
    "        with torch.no_grad():\n",
    "            self.entity_embeddings.weight.data = nn.functional.normalize(\n",
    "                self.entity_embeddings.weight.data, p=2, dim=1\n",
    "            )\n",
    "\n",
    "    def forward(self, subject_indices, relation_indices):\n",
    "        # S x R -> Predicted Object Vector\n",
    "        subj_vecs = self.entity_embeddings(subject_indices)\n",
    "        rel_mats = self.relation_matrices[relation_indices]\n",
    "        \n",
    "        # Tensor Contraction: Vector * Matrix\n",
    "        # b=batch, i=input_dim, j=output_dim\n",
    "        # Equivalent to: \"Logic rules are essentially Einstein summation\" [cite: 933]\n",
    "        pred_vecs = torch.einsum('bi,bij->bj', subj_vecs, rel_mats)\n",
    "        return pred_vecs\n",
    "\n",
    "    def score_all(self, pred_vecs):\n",
    "        # Calculate Dot Product of prediction against ALL entities\n",
    "        # Result: (Batch, Num_Entities) scores\n",
    "        all_emb = self.entity_embeddings.weight\n",
    "        # This computes the similarity (Gram matrix row) for the batch\n",
    "        scores = torch.matmul(pred_vecs, all_emb.T)\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba3624",
   "metadata": {},
   "source": [
    "#### TRAINING (CROSS ENTROPY)\n",
    "#### Maximizing the dot product of the correct triple effectively maximizes the probability P(Object | Subject, Relation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4642541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING (Optimizing Dot Product Probability)...\n",
      "Epoch 0: Loss = 6.1942\n",
      "Epoch 100: Loss = 1.0020\n",
      "Epoch 200: Loss = 0.1504\n",
      "Epoch 300: Loss = 0.0595\n",
      "Epoch 400: Loss = 0.0350\n",
      "Training Complete. Final Loss: 0.0242\n",
      "\n",
      "============================================================\n",
      "VERIFICATION: Embedding Norms (Hubness Check)\n",
      "============================================================\n",
      "Min norm:  1.000000\n",
      "Max norm:  1.000000\n",
      "Mean norm: 1.000000\n",
      "Std norm:  0.000000\n",
      "\n",
      "All norms ≈ 1.0? True\n",
      "\n",
      "Sample Entity Norms:\n",
      "  Asia        : 1.000000\n",
      "  Europe      : 1.000000\n",
      "  Africa      : 1.000000\n",
      "  Americas    : 1.000000\n",
      "  Oceania     : 1.000000\n",
      "  Antarctic   : 1.000000\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "model = TensorLogicDotProduct(NUM_ENTITIES, EMBEDDING_DIM)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) \n",
    "criterion = nn.CrossEntropyLoss() # Softmax + NLL\n",
    "\n",
    "# Prepare Data (Ensuring LongTensor for indices)\n",
    "subjects = torch.tensor([ent_to_idx[t[0]] for t in triples], dtype=torch.long)\n",
    "rels = torch.tensor([rel_to_idx[t[1]] for t in triples], dtype=torch.long)\n",
    "objects = torch.tensor([ent_to_idx[t[2]] for t in triples], dtype=torch.long)\n",
    "\n",
    "print(\"\\nTRAINING (Optimizing Dot Product Probability)...\")\n",
    "\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 1. Predict Vector\n",
    "    pred_vecs = model(subjects, rels)\n",
    "    \n",
    "    # 2. Score against all entities (Dot Product)\n",
    "    logits = model.score_all(pred_vecs)\n",
    "    \n",
    "    # 3. Loss\n",
    "    loss = criterion(logits, objects)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # CRITICAL: Re-normalize embeddings after each update (prevents hubness)\n",
    "    with torch.no_grad():\n",
    "        model.entity_embeddings.weight.data = nn.functional.normalize(\n",
    "            model.entity_embeddings.weight.data, p=2, dim=1\n",
    "        )\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"Training Complete. Final Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ========== VERIFICATION 1: Check all embeddings are unit vectors ==========\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION: Embedding Norms (Hubness Check)\")\n",
    "print(\"=\"*60)\n",
    "norms = torch.norm(model.entity_embeddings.weight, p=2, dim=1)\n",
    "print(f\"Min norm:  {norms.min():.6f}\")\n",
    "print(f\"Max norm:  {norms.max():.6f}\")\n",
    "print(f\"Mean norm: {norms.mean():.6f}\")\n",
    "print(f\"Std norm:  {norms.std():.6f}\")\n",
    "print(f\"\\nAll norms ≈ 1.0? {torch.allclose(norms, torch.ones_like(norms), atol=1e-5)}\")\n",
    "\n",
    "# Show norm distribution for key entities\n",
    "print(\"\\nSample Entity Norms:\")\n",
    "sample_entities = ['Asia', 'Europe', 'Africa', 'Americas', 'Oceania', 'Antarctic']\n",
    "for entity_name in sample_entities:\n",
    "    if entity_name in ent_to_idx:\n",
    "        idx = ent_to_idx[entity_name]\n",
    "        norm = norms[idx].item()\n",
    "        print(f\"  {entity_name:12s}: {norm:.6f}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bbb5d",
   "metadata": {},
   "source": [
    "#### EVALUATING COMPOSITIONALITY\n",
    "#### Reasoning in embedding space can now be carried out by forward or backward chaining over the embedded rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0080bb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference_dot(city_name):\n",
    "    if city_name not in ent_to_idx: return\n",
    "\n",
    "    # 1. Get Embeddings & Matrices\n",
    "    city_idx = torch.tensor([ent_to_idx[city_name]], dtype=torch.long)\n",
    "    city_vec = model.entity_embeddings(city_idx) \n",
    "    \n",
    "    M_CapitalOf = model.relation_matrices[rel_to_idx[\"is_capital_of\"]] \n",
    "    M_LocatedIn = model.relation_matrices[rel_to_idx[\"is_located_in\"]] \n",
    "    \n",
    "    # 2. LOGICAL COMPOSITION (CHAINING)\n",
    "    # City -> (CapitalOf) -> Country -> (LocatedIn) -> Region\n",
    "    country_pred_vec = torch.einsum('bi,ij->bj', city_vec, M_CapitalOf)\n",
    "    region_pred_vec = torch.einsum('bj,jk->bk', country_pred_vec, M_LocatedIn)\n",
    "    \n",
    "    # 3. DOT PRODUCT SEARCH (Maximum Inner Product)\n",
    "    all_emb = model.entity_embeddings.weight\n",
    "    # Dot product of Result vs All Entities\n",
    "    scores = torch.matmul(region_pred_vec, all_emb.T).squeeze()\n",
    "    \n",
    "    # Get Top Predictions\n",
    "    top_indices = torch.argsort(scores, descending=True)[:3]\n",
    "    \n",
    "    print(f\"\\nQuery: What continent is {city_name} in?\")\n",
    "    print(f\"Method: ArgMax( Dot( Vec({city_name}) x M_cap x M_loc, All_Entities ) )\")\n",
    "    for i in top_indices:\n",
    "        name = idx_to_ent[i.item()]\n",
    "        score = scores[i].item()\n",
    "        print(f\"  - {name} (score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d22e6",
   "metadata": {},
   "source": [
    "#### Test on diverse cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a4eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What continent is Tokyo in?\n",
      "Method: ArgMax( Dot( Vec(Tokyo) x M_cap x M_loc, All_Entities ) )\n",
      "  - Asia (score: 147.50)\n",
      "  - Europe (score: 82.79)\n",
      "  - Alofi (score: 25.59)\n",
      "\n",
      "Query: What continent is Berlin in?\n",
      "Method: ArgMax( Dot( Vec(Berlin) x M_cap x M_loc, All_Entities ) )\n",
      "  - Europe (score: 108.62)\n",
      "  - Oceania (score: 50.09)\n",
      "  - Asia (score: 36.31)\n",
      "\n",
      "Query: What continent is Cairo in?\n",
      "Method: ArgMax( Dot( Vec(Cairo) x M_cap x M_loc, All_Entities ) )\n",
      "  - Africa (score: 127.38)\n",
      "  - Oceania (score: 94.68)\n",
      "  - Antarctic (score: 43.71)\n",
      "\n",
      "Query: What continent is Lima in?\n",
      "Method: ArgMax( Dot( Vec(Lima) x M_cap x M_loc, All_Entities ) )\n",
      "  - Americas (score: 110.34)\n",
      "  - Africa (score: 25.26)\n",
      "  - Jamaica (score: 25.22)\n",
      "\n",
      "Query: What continent is Canberra in?\n",
      "Method: ArgMax( Dot( Vec(Canberra) x M_cap x M_loc, All_Entities ) )\n",
      "  - Oceania (score: 178.73)\n",
      "  - Europe (score: 100.39)\n",
      "  - Africa (score: 98.36)\n",
      "\n",
      "Query: What continent is New Delhi in?\n",
      "Method: ArgMax( Dot( Vec(New Delhi) x M_cap x M_loc, All_Entities ) )\n",
      "  - Asia (score: 131.31)\n",
      "  - Europe (score: 53.42)\n",
      "  - Honiara (score: 22.29)\n",
      "\n",
      "Query: What continent is King Edward Point in?\n",
      "Method: ArgMax( Dot( Vec(King Edward Point) x M_cap x M_loc, All_Entities ) )\n",
      "  - Antarctic (score: 90.38)\n",
      "  - Oceania (score: 50.24)\n",
      "  - Europe (score: 37.68)\n"
     ]
    }
   ],
   "source": [
    "test_cities = [\"Tokyo\", \"Berlin\", \"Cairo\", \"Lima\", \"Canberra\", \"New Delhi\", \"King Edward Point\"]\n",
    "for city in test_cities:\n",
    "    test_inference_dot(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71bd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c6c60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef09cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eefef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067acb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
